--- /mnt/data/06_build_bulk_eigenmodes_dataset.py.bak	2025-12-12 11:00:13.424451461 +0000
+++ /mnt/data/06_build_bulk_eigenmodes_dataset.py	2025-12-12 11:00:13.438210278 +0000
@@ -5,28 +5,22 @@
 # OBJETIVO
 #   Recorrer las geometrías emergentes y construir un dataset honesto de modos bulk:
 #     - Llamar a bulk_scalar_solver.py para cada sistema.
-#     - Recopilar pares (Δ_UV, λ_SL) con metadatos (familia, d, etc.).
+#     - Recopilar pares (Delta_UV, lambda_sl) con metadatos (familia, d, ...).
 #
 # ENTRADAS
-#   - runs/emergent_geometry/geometry_emergent/*.h5
-#   - Módulo bulk_scalar_solver.py
+#   - runs/**/geometry_emergent/*.h5
+#   - Módulo bulk_scalar_solver.py (o bulk_scalar_solver_v2 si existe)
 #
-# SALIDAS
+# SALIDAS (IO_CONTRACTS_V1)
 #   runs/bulk_eigenmodes/
 #     bulk_modes_dataset.csv
-#       - Columnas típicas: system_name, family, d, z_dyn, theta, mode_id,
-#         lambda_sl, Delta_UV, etiquetas adicionales.
 #     bulk_modes_meta.json
-#       - Información de configuración, n_modes, seeds, etc.
-#
-# RELACIÓN CON OTROS SCRIPTS
-#   - Consume geometrías de 02_emergent_geometry_engine.py.
-#   - Alimenta al diccionario emergente:
-#       * 07_emergent_lambda_sl_dictionary.py
+#   (Opcional / compat)
+#     --output-json: JSON agregador (por-sistema / por-family-d), útil para stubs Fase XII.
 #
 # HONESTIDAD
-#   - No se aplica ninguna fórmula teórica Δ(Δ-d) al construir el dataset.
-#   - Este script solo computa espectros y los vuelca a datos.
+#   - No se aplica ninguna fórmula teórica Delta(Delta-d).
+#   - lambda_sl son autovalores Sturm–Liouville, NO masas holográficas por defecto.
 #
 # HISTÓRICO
 #   - Anteriormente conocido como: make_fase11_bulk_for_fase12c_v2.py
@@ -34,129 +28,295 @@
 from __future__ import annotations
 
 import argparse
+import csv
 import json
+from dataclasses import dataclass
+from datetime import datetime
 from pathlib import Path
-from typing import Dict, List, Any, Set
+from typing import Any, Dict, List, Optional, Sequence, Set, Tuple
 
 import h5py
 import numpy as np
 
 # Importar el solver v2 con nomenclatura honesta
-# Si el import falla, intenta con el nombre original como fallback
 try:
-    import bulk_scalar_solver_v2 as bss
+    import bulk_scalar_solver_v2 as bss  # type: ignore
 except ImportError:
-    import bulk_scalar_solver as bss
+    import bulk_scalar_solver as bss  # type: ignore
 
 
 def parse_args() -> argparse.Namespace:
     p = argparse.ArgumentParser(
-        description="Construir dataset bulk-emergente para Fase XII.c (v2: nomenclatura λ_SL)"
+        description=(
+            "Construir dataset de modos escalares (CSV canónico) a partir de geometrías emergentes. "
+            "Nomenclatura honesta: lambda_sl (autovalores Sturm–Liouville)."
+        )
     )
     p.add_argument(
         "--geometry-dir",
         type=str,
         required=True,
-        help="Directorio con los .h5 de Fase XI (p.ej. fase11_output_v2/data)",
+        help="Directorio con .h5 de geometría emergente (p.ej. runs/emergent_geometry/geometry_emergent)",
+    )
+
+    # Salidas canónicas
+    p.add_argument(
+        "--output-csv",
+        type=str,
+        default="runs/bulk_eigenmodes/bulk_modes_dataset.csv",
+        help="CSV canónico (default: runs/bulk_eigenmodes/bulk_modes_dataset.csv)",
     )
     p.add_argument(
+        "--output-meta",
+        type=str,
+        default="runs/bulk_eigenmodes/bulk_modes_meta.json",
+        help="JSON meta del dataset (default: runs/bulk_eigenmodes/bulk_modes_meta.json)",
+    )
+
+    # Compat / usos auxiliares (Ising, etc.)
+    p.add_argument(
         "--output-json",
         type=str,
-        required=True,
-        help="Ruta del JSON de salida (p.ej. data_processed/fase11_bulk_for_fase12c.json)",
+        default=None,
+        help=(
+            "(Opcional) JSON agregador por sistema/family-d (compat/aux). "
+            "Recomendado para pipelines legacy o stubs Fase XII."
+        ),
     )
+
     p.add_argument(
         "--n-eigs",
         type=int,
         default=4,
-        help="Número de autovalores/autovectores por geometría",
+        help="Número de autovalores/autovectores por geometría (default: 4)",
     )
-    # NOTA sobre paths de datasets:
-    # Estos defaults asumen el layout de 01_emergent_geometry_v2.py (bulk_truth/*)
-    # Si cambias el layout en Fase XI, ajusta aquí o pasa rutas explícitas.
+
+    # Datasets dentro del HDF5: por defecto, layout emergent (02)
     p.add_argument(
         "--z-dataset",
         type=str,
-        default="bulk_truth/z_grid",
-        help="Ruta al dataset z dentro del HDF5 (default: bulk_truth/z_grid)",
+        default="z_grid",
+        help="Ruta al dataset z dentro del HDF5 (default: z_grid)",
     )
     p.add_argument(
         "--A-dataset",
         type=str,
-        default="bulk_truth/A_truth",
-        help="Ruta al dataset A(z) dentro del HDF5 (default: bulk_truth/A_truth)",
+        default="A_emergent",
+        help="Ruta al dataset A(z) dentro del HDF5 (default: A_emergent)",
     )
     p.add_argument(
         "--f-dataset",
         type=str,
-        default="bulk_truth/f_truth",
-        help="Ruta al dataset f(z) dentro del HDF5 (default: bulk_truth/f_truth)",
+        default="f_emergent",
+        help="Ruta al dataset f(z) dentro del HDF5 (default: f_emergent)",
     )
+
     return p.parse_args()
 
 
-def get_family_and_d(h5_path: Path) -> Dict[str, Any]:
-    """
-    Lee atributos 'family' y 'd' del HDF5, con defaults razonables.
-    """
+def _decode_if_bytes(x: Any) -> Any:
+    if isinstance(x, bytes):
+        return x.decode("utf-8", errors="ignore")
+    return x
+
+
+def read_required_attrs(h5_path: Path) -> Dict[str, Any]:
+    """Lee metadatos críticos. No hace defaults silenciosos."""
     with h5py.File(h5_path, "r") as f:
-        family = f.attrs.get("family", "unknown")
-        if isinstance(family, bytes):
-            family = family.decode("utf-8", errors="ignore")
-        d_attr = f.attrs.get("d", 4)
-        try:
-            d = int(d_attr)
-        except Exception:
-            d = 4
-    return {"family": family, "d": d}
+        # system_name / name
+        system_name = _decode_if_bytes(f.attrs.get("system_name", f.attrs.get("name", h5_path.stem)))
+        family = _decode_if_bytes(f.attrs.get("family", f.attrs.get("family_pred", None)))
+        d = f.attrs.get("d", f.attrs.get("d_pred", None))
+        z_dyn = f.attrs.get("z_dyn", np.nan)
+        theta = f.attrs.get("theta", np.nan)
+
+    if family is None:
+        raise ValueError(f"[{h5_path.name}] Falta attrs 'family' (o 'family_pred')")
+    if d is None:
+        raise ValueError(f"[{h5_path.name}] Falta attrs 'd' (o 'd_pred')")
+    try:
+        d_int = int(d)
+    except Exception as e:
+        raise ValueError(f"[{h5_path.name}] Attr 'd' no es int: {d!r}") from e
+
+    return {
+        "system_name": str(system_name),
+        "family": str(family),
+        "d": d_int,
+        "z_dyn": float(z_dyn) if z_dyn is not None else float("nan"),
+        "theta": float(theta) if theta is not None else float("nan"),
+    }
+
+
+def pick_existing_dataset(h5_path: Path, candidates: Sequence[str]) -> str:
+    """Devuelve el primer dataset existente en el HDF5."""
+    with h5py.File(h5_path, "r") as f:
+        for key in candidates:
+            if key in f:
+                return key
+    raise KeyError(f"[{h5_path.name}] No existe ninguno de los datasets: {list(candidates)}")
+
+
+@dataclass
+class Row:
+    system_name: str
+    family: str
+    d: int
+    z_dyn: float
+    theta: float
+    mode_id: int
+    lambda_sl: float
+    Delta_UV: Optional[float]
+    quality_flag: str
+    is_ground_state: bool
+
+
+def write_csv(rows: List[Row], out_csv: Path) -> None:
+    out_csv.parent.mkdir(parents=True, exist_ok=True)
+    fieldnames = [
+        "system_name",
+        "family",
+        "d",
+        "z_dyn",
+        "theta",
+        "mode_id",
+        "lambda_sl",
+        "Delta_UV",
+        "quality_flag",
+        "is_ground_state",
+    ]
+    with open(out_csv, "w", newline="", encoding="utf-8") as f:
+        w = csv.DictWriter(f, fieldnames=fieldnames)
+        w.writeheader()
+        for r in rows:
+            w.writerow(
+                {
+                    "system_name": r.system_name,
+                    "family": r.family,
+                    "d": r.d,
+                    "z_dyn": "" if np.isnan(r.z_dyn) else f"{r.z_dyn:.12g}",
+                    "theta": "" if np.isnan(r.theta) else f"{r.theta:.12g}",
+                    "mode_id": r.mode_id,
+                    "lambda_sl": f"{r.lambda_sl:.16g}",
+                    "Delta_UV": "" if r.Delta_UV is None else f"{r.Delta_UV:.16g}",
+                    "quality_flag": r.quality_flag,
+                    "is_ground_state": int(r.is_ground_state),
+                }
+            )
+
+
+def build_legacy_json(rows: List[Row]) -> Dict[str, Any]:
+    """Construye un JSON agregador compatible con loaders legacy (por-sistema y por family-d)."""
+    systems: Dict[str, Dict[str, Any]] = {}
+    for r in rows:
+        if r.system_name not in systems:
+            systems[r.system_name] = {
+                "geometry_name": r.system_name,
+                "family": r.family,
+                "d": r.d,
+                "n_modes": 0,
+                "Delta_bulk_uv": [],
+                "lambda_sl_bulk": [],
+                "lambda_source": "bulk_eigenmode",
+            }
+        systems[r.system_name]["n_modes"] += 1
+        systems[r.system_name]["lambda_sl_bulk"].append(float(r.lambda_sl))
+        systems[r.system_name]["Delta_bulk_uv"].append(None if r.Delta_UV is None else float(r.Delta_UV))
+
+    by_family_d: Dict[str, Dict[str, Any]] = {}
+    for sys in systems.values():
+        key = f"{sys['family']}_d{sys['d']}"
+        if key not in by_family_d:
+            by_family_d[key] = {
+                "family": sys["family"],
+                "d": sys["d"],
+                "Delta_bulk_uv": [],
+                "lambda_sl_bulk": [],
+                "geometries": [],
+            }
+        by_family_d[key]["geometries"].append(sys["geometry_name"])
+        by_family_d[key]["Delta_bulk_uv"].extend(sys["Delta_bulk_uv"])
+        by_family_d[key]["lambda_sl_bulk"].extend(sys["lambda_sl_bulk"])
+
+    return {
+        "timestamp": datetime.now().isoformat(),
+        "source": "06_build_bulk_eigenmodes_dataset",
+        "nomenclature_version": "v2_lambda_sl",
+        "systems": list(systems.values()),
+        "by_family_d": by_family_d,
+        "notes": [
+            "Compat JSON: agregado por sistema/family-d.",
+            "lambda_sl son autovalores Sturm–Liouville, NO masas holográficas por defecto.",
+            "Delta_bulk_uv es el exponente UV estimado numéricamente cuando es fiable.",
+        ],
+    }
 
 
-def main():
+def main() -> None:
     args = parse_args()
     geom_dir = Path(args.geometry_dir)
-    out_path = Path(args.output_json)
+    out_csv = Path(args.output_csv)
+    out_meta = Path(args.output_meta)
+    out_json = Path(args.output_json) if args.output_json else None
 
     if not geom_dir.exists() or not geom_dir.is_dir():
-        raise FileNotFoundError(f"--geometry-dir no es un directorio valido: {geom_dir}")
+        raise FileNotFoundError(f"--geometry-dir no es un directorio válido: {geom_dir}")
 
     h5_files = sorted(geom_dir.glob("*.h5"))
     if not h5_files:
         raise FileNotFoundError(f"No se encontraron .h5 en {geom_dir}")
 
-    systems: List[Dict[str, Any]] = []
-    compat_used_keys: Set[str] = set()  # Rastrear qué claves se usaron (v2 vs legacy)
-
     print("=" * 70)
-    print("FASE XI → XII.c  —  DATASET BULK-EMERGENTE (eigenmodes)")
-    print("Versión 2: Nomenclatura honesta (λ_SL en lugar de m²L²)")
+    print("DATASET BULK EIGENMODES (CSV canónico) — CUERDAS Bloque B")
+    print("Nomenclatura honesta: lambda_sl (autovalores Sturm–Liouville)")
     print("=" * 70)
     print(f"Geometrías desde: {geom_dir}")
     print(f"N_max modos por geometría: {args.n_eigs}")
-    print(f"Datasets: z={args.z_dataset}, A={args.A_dataset}, f={args.f_dataset}")
+    print(f"Salida CSV:  {out_csv}")
+    print(f"Salida META: {out_meta}")
+    if out_json:
+        print(f"Salida JSON (compat): {out_json}")
     print("=" * 70)
 
+    rows: List[Row] = []
+    failed: List[Dict[str, Any]] = []
+    compat_used_keys: Set[str] = set()
+
     for h5_path in h5_files:
-        geom_name = h5_path.stem
-        meta = get_family_and_d(h5_path)
+        meta = read_required_attrs(h5_path)
+        system_name = meta["system_name"]
         family = meta["family"]
         d = meta["d"]
+        z_dyn = meta["z_dyn"]
+        theta = meta["theta"]
 
-        print(f"\n>> Procesando geometría: {geom_name} (family={family}, d={d})")
+        # Resolver datasets: usar el argumento, con fallbacks típicos
+        try:
+            z_ds = pick_existing_dataset(h5_path, [args.z_dataset, "z_grid", "bulk_truth/z_grid"])
+            A_ds = pick_existing_dataset(h5_path, [args.A_dataset, "A_emergent", "A_of_z", "bulk_truth/A_truth"])
+            f_ds = pick_existing_dataset(h5_path, [args.f_dataset, "f_emergent", "f_of_z", "bulk_truth/f_truth"])
+        except Exception as e:
+            failed.append({"system_name": system_name, "file": str(h5_path), "stage": "dataset_resolve", "error": str(e)})
+            print(f"\n>> {system_name}: [FAIL] no se pudieron resolver datasets: {e}")
+            continue
+
+        print(f"\n>> Procesando: {system_name} (family={family}, d={d})")
+        print(f"   datasets: z={z_ds}, A={A_ds}, f={f_ds}")
 
         try:
             spec = bss.solve_geometry(
                 h5_path=h5_path,
                 n_eigs=args.n_eigs,
-                z_dataset=args.z_dataset,
-                A_dataset=args.A_dataset,
-                f_dataset=args.f_dataset,
+                z_dataset=z_ds,
+                A_dataset=A_ds,
+                f_dataset=f_ds,
             )
         except Exception as e:
-            print(f"   [WARN] Fallo solver en {geom_name}: {e}")
+            failed.append({"system_name": system_name, "file": str(h5_path), "stage": "solver", "error": str(e)})
+            print(f"   [WARN] Fallo solver en {system_name}: {e}")
             continue
 
-        # Compatibilidad: aceptamos tanto lambda_sl (nuevo) como m2L2 (legacy)
-        # Rastreamos qué clave se usó para trazabilidad
+        # Compatibilidad: lambda_sl (nuevo) o m2L2* (legacy)
         if "lambda_sl" in spec:
             lambda_list = spec["lambda_sl"]
             used_key = "lambda_sl"
@@ -167,100 +327,104 @@
             lambda_list = spec["m2L2_legacy"]
             used_key = "m2L2_legacy"
         else:
-            print(f"   [WARN] Espectro bulk sin 'lambda_sl' ni 'm2L2(_legacy)' en {geom_name}")
+            failed.append({"system_name": system_name, "file": str(h5_path), "stage": "parse_spec", "error": "missing lambda_sl/m2L2"})
+            print(f"   [WARN] Espectro sin 'lambda_sl' ni 'm2L2(_legacy)' en {system_name}")
             continue
-        
+
         compat_used_keys.add(used_key)
-            
-        Delta_uv_list = spec["uv_exponents"]
+        Delta_uv_list = spec.get("uv_exponents", [])
 
-        # Filtrar parejas razonables: necesitamos λ>0 y un exponente UV finito
-        lambda_clean: List[float] = []
-        Delta_clean: List[float] = []
-        
-        for lam, Delta_uv in zip(lambda_list, Delta_uv_list):
+        n_added = 0
+        for mode_id, lam in enumerate(lambda_list):
             if lam is None:
                 continue
-            if lam <= 0:
+            try:
+                lam_f = float(lam)
+            except Exception:
                 continue
-            if Delta_uv is None or not np.isfinite(Delta_uv):
+            if not np.isfinite(lam_f) or lam_f <= 0:
                 continue
-            lambda_clean.append(float(lam))
-            Delta_clean.append(float(Delta_uv))
 
-        if not lambda_clean:
-            print("   [WARN] Sin modos escalares con λ_SL>0 y Delta_UV fiable; se omite.")
-            continue
+            Delta_uv: Optional[float] = None
+            quality = "ok"
+            if mode_id < len(Delta_uv_list):
+                dv = Delta_uv_list[mode_id]
+                if dv is None or (isinstance(dv, float) and not np.isfinite(dv)):
+                    Delta_uv = None
+                    quality = "uv_unreliable"
+                else:
+                    try:
+                        Delta_uv = float(dv)
+                    except Exception:
+                        Delta_uv = None
+                        quality = "uv_unreliable"
+            else:
+                Delta_uv = None
+                quality = "uv_missing"
+
+            rows.append(
+                Row(
+                    system_name=system_name,
+                    family=family,
+                    d=d,
+                    z_dyn=z_dyn,
+                    theta=theta,
+                    mode_id=int(mode_id),
+                    lambda_sl=lam_f,
+                    Delta_UV=Delta_uv,
+                    quality_flag=quality,
+                    is_ground_state=(mode_id == 0),
+                )
+            )
+            n_added += 1
 
-        # NOMENCLATURA v2: lambda_sl_bulk en lugar de m2L2_bulk
-        system_entry = {
-            "geometry_name": geom_name,
-            "family": family,
-            "d": d,
-            "n_modes": len(lambda_clean),
-            "Delta_bulk_uv": Delta_clean,
-            "lambda_sl_bulk": lambda_clean,      # NUEVO: antes era m2L2_bulk
-            "lambda_source": "bulk_eigenmode",   # NUEVO: antes era m2L2_method
-        }
-        systems.append(system_entry)
-
-        print(
-            f"   OK: {len(lambda_clean)} modos limpios. "
-            f"Ejemplo: λ_SL={lambda_clean[0]:.4f}, Delta_UV={Delta_clean[0]:.4f}"
-        )
+        if n_added == 0:
+            print("   [WARN] Sin modos con lambda_sl>0; se omite sistema.")
+        else:
+            # Mostrar primer modo no-nulo
+            ex = next((r for r in rows if r.system_name == system_name), None)
+            if ex is not None:
+                dv = "(vacío)" if ex.Delta_UV is None else f"{ex.Delta_UV:.6f}"
+                print(f"   OK: {n_added} filas. Ejemplo: lambda_sl={ex.lambda_sl:.6f}, Delta_UV={dv}")
 
-    # Agrupar por (family, d) para que XII.c lo tenga fácil
-    by_system_key: Dict[str, Dict[str, Any]] = {}
-    for sys in systems:
-        key = f"{sys['family']}_d{sys['d']}"
-        if key not in by_system_key:
-            by_system_key[key] = {
-                "family": sys["family"],
-                "d": sys["d"],
-                "Delta_bulk_uv": [],
-                "lambda_sl_bulk": [],    # NUEVO: antes era m2L2_bulk
-                "geometries": [],
-            }
-        by_system_key[key]["Delta_bulk_uv"].extend(sys["Delta_bulk_uv"])
-        by_system_key[key]["lambda_sl_bulk"].extend(sys["lambda_sl_bulk"])
-        by_system_key[key]["geometries"].append(sys["geometry_name"])
-
-    summary = {
-        "source": "fase11_bulk_scalar_solver_v2",
-        "solver_module": getattr(bss, "__name__", "bulk_scalar_solver_v2"),
-        "solver_version_note": "lambda_sl (v2), alias m2L2_legacy solo para lectura",
-        "geometry_dir": str(geom_dir),
-        "n_geometries": len(systems),
+    # Escribir CSV + META
+    write_csv(rows, out_csv)
+
+    meta = {
+        "timestamp": datetime.now().isoformat(),
         "nomenclature_version": "v2_lambda_sl",
-        # Trazabilidad: qué claves se usaron (idealmente solo "lambda_sl")
+        "geometry_dir": str(geom_dir),
+        "n_geometries_scanned": len(h5_files),
+        "n_geometries_solved": len(sorted(set(r.system_name for r in rows))),
+        "n_rows": len(rows),
+        "n_eigs": int(args.n_eigs),
+        "datasets_requested": {"z": args.z_dataset, "A": args.A_dataset, "f": args.f_dataset},
+        "solver_module": getattr(bss, "__name__", "bulk_scalar_solver"),
         "compat_used_keys": sorted(list(compat_used_keys)),
         "all_v2_clean": compat_used_keys == {"lambda_sl"},
-        "systems": systems,
-        "by_family_d": by_system_key,
+        "failed_systems": failed,
         "notes": [
-            "Dataset para XII.c construido a partir de modos escalares bulk (eigenmodes).",
-            "NOMENCLATURA v2: Los valores son λ_SL (autovalores Sturm–Liouville),",
-            "NO deben interpretarse automáticamente como m² holográfico de Maldacena.",
-            "No se ha usado m^2L^2 = Delta(Delta-d) en el camino de datos.",
-            "Delta_bulk_uv es el exponente UV estimado numéricamente desde el modo escalar.",
-            "La relación λ_SL ↔ Δ es algo a DESCUBRIR, no a asumir.",
+            "CSV canónico para 07: system_name,family,d,mode_id,lambda_sl,Delta_UV (+ opcionales).",
+            "lambda_sl son autovalores Sturm–Liouville (NO masas por defecto).",
+            "quality_flag marca fiabilidad de Delta_UV (uv_unreliable/uv_missing).",
         ],
     }
-    
-    # Avisar si se usaron claves legacy
-    if compat_used_keys != {"lambda_sl"}:
-        legacy_keys = compat_used_keys - {"lambda_sl"}
-        print(f"\n   [INFO] Se usaron claves legacy: {legacy_keys}")
-        print("          Considera regenerar los HDF5 con bulk_scalar_solver_v2.py")
-        summary["notes"].append(
-            f"COMPAT: Se usaron claves legacy {list(legacy_keys)}. "
-            "Regenerar HDF5 con solver v2 para pipeline 100% limpio."
-        )
 
-    out_path.parent.mkdir(parents=True, exist_ok=True)
-    out_path.write_text(json.dumps(summary, indent=2))
+    out_meta.parent.mkdir(parents=True, exist_ok=True)
+    out_meta.write_text(json.dumps(meta, indent=2, ensure_ascii=False))
+
+    if out_json is not None:
+        out_json.parent.mkdir(parents=True, exist_ok=True)
+        out_json.write_text(json.dumps(build_legacy_json(rows), indent=2, ensure_ascii=False))
+
     print("\n" + "=" * 70)
-    print(f"Dataset bulk-emergente (v2) guardado en: {out_path}")
+    print("[OK] Dataset bulk-eigenmodes generado")
+    print(f"  CSV :  {out_csv}")
+    print(f"  META:  {out_meta}")
+    if out_json:
+        print(f"  JSON:  {out_json}")
+    if failed:
+        print(f"  WARN: {len(failed)} sistemas fallaron (ver bulk_modes_meta.json)")
     print("=" * 70)
 
 
